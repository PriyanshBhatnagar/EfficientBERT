{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from models import *\n",
    "from collections import namedtuple\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.utils.prune as prune\n",
    "import tokenization\n",
    "import models\n",
    "import optim as optim\n",
    "import train_org as train\n",
    "from utils import set_seeds, get_device, truncate_tokens_pair\n",
    "from classify import dataset_class, Tokenizing, AddSpecialTokensWithTruncation, TokenIndexing, Classifier\n",
    "from classify_SVM import dataset_class, Tokenizing, AddSpecialTokensWithTruncation, TokenIndexing, Classifier_feats\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Const Values for the file locations\n",
    "task='mrpc'\n",
    "\n",
    "train_cfg= f'config/train_{task}.json'\n",
    "model_cfg='config/bert_base.json'\n",
    "\n",
    "train_data_file = f'C:/Users/Vyshnavi S K/Downloads/GLUE-baselines-master/GLUE-baselines-master/glue_data/MRPC/train.tsv'\n",
    "test_data_file=f'C:/Users/Vyshnavi S K/Downloads/GLUE-baselines-master/GLUE-baselines-master/glue_data/MRPC/dev.tsv'\n",
    "\n",
    "pretrain_file = 'C:/Users/Vyshnavi S K/Downloads/pytorch-pretrained-BERT-master/pytorch-pretrained-BERT-master/uncased_L-12_H-768_A-12/bert_model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Const values for the network \n",
    "max_len=128\n",
    "\n",
    "#vocab='./PRE_TRAINED_MODEL/vocab.txt'\n",
    "vocab = 'C:/Users/Vyshnavi S K/Downloads/vocab.txt'\n",
    "save_dir = './SAVE'\n",
    "model_file = 'C:/Users/Vyshnavi S K/Downloads/finetuned_mrpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = train.Config.from_json(train_cfg)\n",
    "model_cfg = models.Config.from_json(model_cfg)\n",
    "set_seeds(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(vocab_size=30522, dim=768, n_layers=12, n_heads=12, dim_ff=3072, p_drop_hidden=0.1, p_drop_attn=0.1, max_len=512, n_segments=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING - TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab, do_lower_case=True)\n",
    "TaskDataset = dataset_class(task) # task dataset class according to the task\n",
    "pipeline = [Tokenizing(tokenizer.convert_to_unicode, tokenizer.tokenize),\n",
    "            AddSpecialTokensWithTruncation(max_len),\n",
    "            TokenIndexing(tokenizer.convert_tokens_to_ids,\n",
    "                            TaskDataset.labels, max_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaskDataset(train_data_file, pipeline)\n",
    "test_dataset = TaskDataset(test_data_file, pipeline)\n",
    "train_data_iter = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "test_data_iter = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668 408\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL - BERT - pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batch):\n",
    "    input_ids, segment_ids, input_mask, label_id = batch\n",
    "    logits = model(input_ids, segment_ids, input_mask)\n",
    "    _, label_pred = logits.max(1)\n",
    "    result = (label_pred == label_id).float() #.cpu().numpy()\n",
    "    accuracy = result.mean()\n",
    "    return accuracy, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, batch, global_step): # make sure loss is a scalar tensor\n",
    "    input_ids, segment_ids, input_mask, label_id = batch\n",
    "    logits = model(input_ids, segment_ids, input_mask)\n",
    "    loss = criterion(logits, label_id)\n",
    "    return loss\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (1 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "trainer = train.Trainer(cfg, model, train_data_iter, test_data_iter, optim.optim4GPU(cfg, model),save_dir, get_device())\n",
    "trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter(acc=1.000): 100%|██████████| 102/102 [00:18<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867647111415863\n"
     ]
    }
   ],
   "source": [
    "results = trainer.eval(evaluate)\n",
    "total_accuracy = torch.cat(results).mean().item()\n",
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIFFERENT CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (1 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading features from BERT model\n",
    "model_feats = Classifier_feats(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "trainer = train.Trainer(cfg, model_feats, train_data_iter, test_data_iter, optim.optim4GPU(cfg, model_feats),save_dir, get_device())\n",
    "trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, data_iter, device):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            input_ids, segment_ids, input_mask, label_id = [tensor.to(device) for tensor in batch]\n",
    "            features = model(input_ids, segment_ids, input_mask)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(label_id.cpu().numpy())\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    return all_features, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_features, train_labels = extract_features(model_feats, train_data_iter, device)\n",
    "test_features, test_labels = extract_features(model_feats, test_data_iter, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(features, labels,classifier):\n",
    "    if classifier=='svm':\n",
    "        svm = SVC(kernel='linear',random_state=42)\n",
    "        svm.fit(features, labels)\n",
    "        return svm\n",
    "    elif classifier=='rf':\n",
    "        rf = RandomForestClassifier(random_state = 42)\n",
    "        rf.fit(features, labels)\n",
    "        return rf\n",
    "    elif classifier=='boosting':\n",
    "        adb = AdaBoostClassifier(n_estimators=100, random_state = 42)\n",
    "        adb.fit(features, labels)\n",
    "        return adb\n",
    "    else:\n",
    "        gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "        gb.fit(features,labels)\n",
    "        return gb\n",
    "    \n",
    "\n",
    "def evaluate_classifier(classifier, features, labels):\n",
    "    predictions = classifier.predict(features)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.875\n",
      "Random Forest Accuracy: 0.8676470588235294\n",
      "AdaBoost Accuracy: 0.8529411764705882\n",
      "Gradient Boosting Accuracy: 0.8676470588235294\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = train_classifier(train_features, train_labels, 'svm')\n",
    "rf_classifier = train_classifier(train_features, train_labels, 'rf')\n",
    "adb_classifier = train_classifier(train_features, train_labels, 'boosting')\n",
    "gb_classifier = train_classifier(train_features, train_labels, 'gb')\n",
    "\n",
    "svm_accuracy = evaluate_classifier(svm_classifier, test_features, test_labels)\n",
    "rf_accuracy = evaluate_classifier(rf_classifier, test_features, test_labels)\n",
    "adb_accuracy = evaluate_classifier(adb_classifier, test_features, test_labels)\n",
    "gb_accuracy = evaluate_classifier(gb_classifier,test_features, test_labels)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"AdaBoost Accuracy:\", adb_accuracy)\n",
    "print(\"Gradient Boosting Accuracy:\", gb_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULLY CONNECTED COMPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"Implementation of the gelu activation function by Hugging Face\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCCompression(nn.Module):\n",
    "    def __init__(self, cfg, k):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.fc1_u = nn.Linear(768, k, bias = False)\n",
    "        self.fc1_vs = nn.Linear(k, 3072)\n",
    "        \n",
    "        self.fc2_u = nn.Linear(3072, k)\n",
    "        self.fc2_vs = nn.Linear(k, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = gelu(self.fc1_vs(self.fc1_u(x)))\n",
    "        out = self.fc2_vs(self.fc2_u(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_matrix(mat, k):\n",
    "    U, s, VT = np.linalg.svd(mat, full_matrices=False)\n",
    "    Sigma = np.diag(s[:k])\n",
    "    U_truncated = U[:, :k]\n",
    "    VT_truncated = VT[:k, :]\n",
    "    return U_truncated, Sigma, VT_truncated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomp_model_func(decomp_trainer, rank):\n",
    "    for b in range(len(trainer.model.transformer.blocks)):\n",
    "        \n",
    "        block = trainer.model.transformer.blocks[b]\n",
    "        target_block = decomp_trainer.model.transformer.blocks[b]\n",
    "\n",
    "        target_block.pwff = FCCompression(model_cfg, rank[b])\n",
    "        \n",
    "        fc1_U, fc1_Sigma, fc1_V = decompose_matrix(block.pwff.fc1.weight.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.pwff.fc1_u.weight.data = torch.from_numpy(fc1_U).T.cuda().contiguous()\n",
    "        target_block.pwff.fc1_vs.weight.data = torch.from_numpy(fc1_Sigma@fc1_V).T.cuda().contiguous()\n",
    "        target_block.pwff.fc1_vs.bias.data = block.pwff.fc1.bias\n",
    "        \n",
    "        fc2_U, fc2_Sigma, fc2_V = decompose_matrix(block.pwff.fc2.weight.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.pwff.fc2_u.weight.data = torch.from_numpy(fc2_U).T.cuda().contiguous()\n",
    "        target_block.pwff.fc2_vs.weight.data = torch.from_numpy(fc2_Sigma@fc2_V).T.cuda().contiguous()\n",
    "        target_block.pwff.fc2_vs.bias.data = block.pwff.fc2.bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (1 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "trainer = train.Trainer(cfg, model, train_data_iter, test_data_iter, optim.optim4GPU(cfg, model),save_dir, get_device())\n",
    "trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (1 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomp_model = Classifier(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "decomp_trainer = train.Trainer(cfg, decomp_model, train_data_iter, test_data_iter, optim.optim4GPU(cfg, decomp_model),save_dir, get_device())\n",
    "decomp_trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [300]*12\n",
    "rank[0] = rank[1] = 384\n",
    "decomp_model_func(decomp_trainer, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48893229166666663\n"
     ]
    }
   ],
   "source": [
    "compression = 0\n",
    "for r in rank:\n",
    "    compression += (3072*r + 768*r)\n",
    "\n",
    "compression = 1 - compression/ (12*768*3072)\n",
    "print(compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tune after compression - TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter(acc=0.750): 100%|██████████| 102/102 [00:16<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7279412150382996\n"
     ]
    }
   ],
   "source": [
    "# Model performance after FC Compression\n",
    "results = decomp_trainer.eval(evaluate)\n",
    "total_accuracy = torch.cat(results).mean().item()\n",
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (1 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading features from BERT model\n",
    "model_feats = Classifier_feats(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "trainer = train.Trainer(cfg, model_feats, train_data_iter, test_data_iter, optim.optim4GPU(cfg, model_feats),save_dir, get_device())\n",
    "trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (1 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomp_model = Classifier(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "decomp_trainer = train.Trainer(cfg, decomp_model, train_data_iter, test_data_iter, optim.optim4GPU(cfg, decomp_model),save_dir, get_device())\n",
    "decomp_trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [300]*12\n",
    "rank[0] = rank[1] = 384\n",
    "decomp_model_func(decomp_trainer, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48893229166666663\n"
     ]
    }
   ],
   "source": [
    "compression = 0\n",
    "for r in rank:\n",
    "    compression += (3072*r + 768*r)\n",
    "\n",
    "compression = 1 - compression/ (12*768*3072)\n",
    "print(compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINETUNE _ TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "decomp_model.to(device)\n",
    "train_features, train_labels = extract_features(decomp_model, train_data_iter, device)\n",
    "test_features, test_labels = extract_features(decomp_model, test_data_iter, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.7230392156862745\n",
      "Random Forest Accuracy: 0.7034313725490197\n",
      "AdaBoost Accuracy: 0.7205882352941176\n",
      "Gradient Boosting Accuracy: 0.7107843137254902\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = train_classifier(train_features, train_labels, 'svm')\n",
    "rf_classifier = train_classifier(train_features, train_labels, 'rf')\n",
    "adb_classifier = train_classifier(train_features, train_labels, 'boosting')\n",
    "gb_classifier = train_classifier(train_features, train_labels, 'gb')\n",
    "\n",
    "svm_accuracy = evaluate_classifier(svm_classifier, test_features, test_labels)\n",
    "rf_accuracy = evaluate_classifier(rf_classifier, test_features, test_labels)\n",
    "adb_accuracy = evaluate_classifier(adb_classifier, test_features, test_labels)\n",
    "gb_accuracy = evaluate_classifier(gb_classifier,test_features, test_labels)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"AdaBoost Accuracy:\", adb_accuracy)\n",
    "print(\"Gradient Boosting Accuracy:\", gb_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadProjection(nn.Module):\n",
    "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
    "    def __init__(self, cfg, k):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
    "        \n",
    "        self.proj_q_u = nn.Linear(768,k)\n",
    "        self.proj_q_vs = nn.Linear(k, 768, bias = False)\n",
    "        \n",
    "        self.proj_k_u = nn.Linear(768,k)\n",
    "        self.proj_k_vs = nn.Linear(k, 768, bias = False)\n",
    "        \n",
    "        self.proj_v_u = nn.Linear(768,k)\n",
    "        self.proj_v_vs = nn.Linear(k, 768, bias = False)\n",
    "            \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        \"\"\"\n",
    "    \n",
    "        q = self.proj_q_u(self.proj_q_vs(x))\n",
    "        k = self.proj_k_u(self.proj_k_vs(x))\n",
    "        v = self.proj_v_u(self.proj_v_vs(x))\n",
    "        \n",
    "        q, k, v = (split_last(x, (12, -1)).transpose(1, 2) for x in [q, k, v])\n",
    "        \n",
    "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :].float()\n",
    "            scores -= 10000.0 * (1.0 - mask)\n",
    "\n",
    "        scores = self.drop(F.softmax(scores, dim=-1))\n",
    "        h = (scores @ v).transpose(1, 2).contiguous()\n",
    "        h = merge_last(h, 2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomp_proj(decom_trainer, rank):\n",
    "    for b in range(len(trainer.model.transformer.blocks)):\n",
    "        \n",
    "        block = trainer.model.transformer.blocks[b]\n",
    "        target_block = decomp_trainer.model.transformer.blocks[b]\n",
    "\n",
    "        target_block.attn = MultiHeadProjection(model_cfg, rank[b]) #, thres = decomp_trainer.model.thres\n",
    "        \n",
    "        proj_q_u, proj_q_s, proj_q_v = decompose_matrix(block.attn.proj_q.weight.T.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.attn.proj_q_u.weight.data = torch.from_numpy(proj_q_u).cuda().contiguous()\n",
    "        target_block.attn.proj_q_vs.weight.data = torch.from_numpy(proj_q_s@proj_q_v).cuda().contiguous()\n",
    "        target_block.attn.proj_q_u.bias.data = block.attn.proj_q.bias\n",
    "        \n",
    "        proj_k_u, proj_k_s, proj_k_v = decompose_matrix(block.attn.proj_k.weight.T.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.attn.proj_k_u.weight.data = torch.from_numpy(proj_k_u).cuda().contiguous()\n",
    "        target_block.attn.proj_k_vs.weight.data = torch.from_numpy(proj_k_s@proj_k_v).cuda().contiguous()\n",
    "        target_block.attn.proj_k_u.bias.data = block.attn.proj_k.bias\n",
    "        \n",
    "        proj_v_u, proj_v_s, proj_v_v = decompose_matrix(block.attn.proj_v.weight.T.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.attn.proj_v_u.weight.data = torch.from_numpy(proj_v_u).cuda().contiguous()\n",
    "        target_block.attn.proj_v_vs.weight.data = torch.from_numpy(proj_v_s@proj_v_v).cuda().contiguous()\n",
    "        target_block.attn.proj_v_u.bias.data = block.attn.proj_v.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [225]*12\n",
    "rank[0] = rank[1] = 384\n",
    "decomp_proj(decomp_trainer, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34505208333333337\n"
     ]
    }
   ],
   "source": [
    "cr = [(2*x*768) / (768*768) for x in rank]\n",
    "print(1 - np.mean(cr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINETUNE - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter(acc=0.750): 100%|██████████| 102/102 [00:16<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6887255311012268\n"
     ]
    }
   ],
   "source": [
    "# Model performance after Attn Compression\n",
    "results = decomp_trainer.eval(evaluate)\n",
    "total_accuracy = torch.cat(results).mean().item()\n",
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "decomp_model.to(device)\n",
    "train_features, train_labels = extract_features(decomp_model, train_data_iter, device)\n",
    "test_features, test_labels = extract_features(decomp_model, test_data_iter, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.6838235294117647\n",
      "Random Forest Accuracy: 0.6102941176470589\n",
      "AdaBoost Accuracy: 0.6862745098039216\n",
      "Gradient Boosting Accuracy: 0.6911764705882353\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = train_classifier(train_features, train_labels, 'svm')\n",
    "rf_classifier = train_classifier(train_features, train_labels, 'rf')\n",
    "adb_classifier = train_classifier(train_features, train_labels, 'boosting')\n",
    "gb_classifier = train_classifier(train_features, train_labels, 'gb')\n",
    "\n",
    "svm_accuracy = evaluate_classifier(svm_classifier, test_features, test_labels)\n",
    "rf_accuracy = evaluate_classifier(rf_classifier, test_features, test_labels)\n",
    "adb_accuracy = evaluate_classifier(adb_classifier, test_features, test_labels)\n",
    "gb_accuracy = evaluate_classifier(gb_classifier,test_features, test_labels)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"AdaBoost Accuracy:\", adb_accuracy)\n",
    "print(\"Gradient Boosting Accuracy:\", gb_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TUNING CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_classifiers(classifier, param_grid, features, labels):\n",
    "    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "    grid_search.fit(features, labels)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def tune_and_train_classifier(features, labels, classifier, kernels=None):\n",
    "    if classifier == 'svm':\n",
    "        if kernels is None:\n",
    "            kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "        \n",
    "        param_grid = {'C': [0.1, 1, 10]}\n",
    "        \n",
    "        if 'poly' in kernels:\n",
    "            param_grid['degree'] = [2, 3, 4]\n",
    "        if 'rbf' in kernels:\n",
    "            param_grid['gamma'] = ['scale', 'auto']\n",
    "        if 'sigmoid' in kernels:\n",
    "            param_grid['coef0'] = [0.0, 0.5, 1.0]\n",
    "        \n",
    "        tuned_classifier = tune_classifiers(SVC(), param_grid, features, labels)\n",
    "    elif classifier == 'rf':\n",
    "        param_grid = {'n_estimators': [50, 100, 200]}\n",
    "        tuned_classifier = tune_classifiers(RandomForestClassifier(random_state=42), param_grid, features, labels)\n",
    "    elif classifier == 'boosting':\n",
    "        param_grid = {'n_estimators': [50, 100, 200]}\n",
    "        tuned_classifier = tune_classifiers(AdaBoostClassifier(random_state=42), param_grid, features, labels)\n",
    "    else:\n",
    "        param_grid = {'n_estimators': [50, 100, 200]}\n",
    "        tuned_classifier = tune_classifiers(GradientBoostingClassifier(random_state=42), param_grid, features, labels)\n",
    "    \n",
    "    tuned_classifier.fit(features, labels)\n",
    "    return tuned_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6936274509803921\n"
     ]
    }
   ],
   "source": [
    "tuned_svm_classifier = tune_and_train_classifier(train_features, train_labels, 'svm', kernels=['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "svm_accuracy = evaluate_classifier(tuned_svm_classifier, test_features, test_labels)\n",
    "print(svm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.6029411764705882\n"
     ]
    }
   ],
   "source": [
    "tuned_rf_classifier = tune_and_train_classifier(train_features, train_labels, 'rf')\n",
    "rf_accuracy = evaluate_classifier(tuned_rf_classifier, test_features, test_labels)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vyshnavi S K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.6838235294117647\n"
     ]
    }
   ],
   "source": [
    "tuned_adb_classifier = tune_and_train_classifier(train_features, train_labels, 'boosting')\n",
    "adb_accuracy = evaluate_classifier(tuned_adb_classifier, test_features, test_labels)\n",
    "print(\"AdaBoost Accuracy:\", adb_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.6887254901960784\n"
     ]
    }
   ],
   "source": [
    "tuned_gb_classifier = tune_and_train_classifier(train_features, train_labels, 'gb')\n",
    "gb_accuracy = evaluate_classifier(tuned_gb_classifier, test_features, test_labels)\n",
    "print(\"Gradient Boosting Accuracy:\", gb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
