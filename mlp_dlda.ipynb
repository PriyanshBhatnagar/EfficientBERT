{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "YsHmXVsqYO8M"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep LDA -\n",
        "\n",
        "https://github.com/VahidooX/DeepLDA/tree/master\n",
        "\n",
        "https://github.com/tchaton/DeepLDA/tree/master\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V189FJS2duS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class LDA_Loss(nn.Module):\n",
        "    def __init__(self, n_components, margin):\n",
        "        super(LDA_Loss, self).__init__()\n",
        "        self.n_components = n_components\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, y_true, y_pred):\n",
        "        r = 1e-4\n",
        "\n",
        "        # Initialize groups\n",
        "        groups = torch.unique(y_true)\n",
        "\n",
        "        def compute_cov(group, Xt, yt):\n",
        "            Xgt = Xt[yt == group]\n",
        "            Xgt_bar = Xgt - torch.mean(Xgt, axis=0)\n",
        "            m = float(Xgt_bar.shape[0])\n",
        "            return (1.0 / (m - 1)) * torch.matmul(Xgt_bar.T, Xgt_bar)\n",
        "\n",
        "        # Scan over groups\n",
        "        covs_t = torch.stack([compute_cov(group, y_pred, y_true) for group in groups])\n",
        "\n",
        "        # Compute average covariance matrix (within scatter)\n",
        "        Sw_t = torch.mean(covs_t, axis=0)\n",
        "\n",
        "        # Compute total scatter\n",
        "        Xt_bar = y_pred - torch.mean(y_pred, axis=0)\n",
        "        m = float(Xt_bar.shape[0])\n",
        "        St_t = (1.0 / (m - 1)) * torch.matmul(Xt_bar.T, Xt_bar)\n",
        "\n",
        "        # Compute between scatter\n",
        "        Sb_t = St_t - Sw_t\n",
        "\n",
        "        # Cope for numerical instability (regularize)\n",
        "        Sw_t += torch.eye(Sw_t.shape[0]) * r\n",
        "\n",
        "        # Compute eigenvalues\n",
        "        evals_t = torch.linalg.eigvalsh(Sb_t, UPLO='U')  # Use UPLO='U' for upper triangular portion\n",
        "\n",
        "        # Get top eigenvalues\n",
        "        top_k_evals = evals_t[-self.n_components:]\n",
        "\n",
        "        # Maximize variance between classes\n",
        "        thresh = torch.min(top_k_evals) + self.margin\n",
        "        top_k_evals = top_k_evals[top_k_evals <= thresh]\n",
        "        costs = torch.mean(top_k_evals)\n",
        "\n",
        "        return -costs\n"
      ],
      "metadata": {
        "id": "T2rIhIDeYQz_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lda_prune_layers(model, x_train, y_train, x_test, y_test, n_components, margin):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Extract features\n",
        "        x_train_features = model(x_train)\n",
        "        x_test_features = model(x_test)\n",
        "    print(x_train.shape,x_train_features.shape)\n",
        "\n",
        "    # Training and testing of SVM with linear kernel on the new features\n",
        "    [train_acc, test_acc] = svm_classify(x_train_features.numpy(), y_train.numpy(), x_test_features.numpy(), y_test.numpy(), C=0.1)\n",
        "\n",
        "    print(\"Accuracy on train data is:\", train_acc * 100.0)\n",
        "    print(\"Accuracy on test data is:\", test_acc * 100.0)\n",
        "    return test_acc"
      ],
      "metadata": {
        "id": "aUzR_zMTYWK8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_classify(x_train, y_train, x_test, y_test, C):\n",
        "    print('Training SVM...')\n",
        "    clf = svm.LinearSVC(C=C, dual=False)\n",
        "    clf.fit(x_train, y_train.ravel())\n",
        "\n",
        "    train_predictions = clf.predict(x_train)\n",
        "    train_acc = accuracy_score(y_train, train_predictions)\n",
        "\n",
        "    test_predictions = clf.predict(x_test)\n",
        "    test_acc = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "    return [train_acc, test_acc]"
      ],
      "metadata": {
        "id": "mCk-AV_BYYtm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6rVppnM8YbQI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outdim_size = 10\n",
        "n_components = 9\n",
        "margin = 1.0\n"
      ],
      "metadata": {
        "id": "LZNi58s0Yzj5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "mnist = fetch_openml(\"mnist_784\")\n",
        "X = mnist.data.astype('float32') / 255\n",
        "y = mnist.target.astype('int64')\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "x_train = x_train.values\n",
        "x_test = x_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpUqiAx0Y2s1",
        "outputId": "ee3db408-80f1-405e-eced-c986638ef5bf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def ada_boost_classify(x_train, y_train, x_test, y_test, n_estimators):\n",
        "    print('Training AdaBoost classifier...')\n",
        "    clf = AdaBoostClassifier(n_estimators=n_estimators)\n",
        "    clf.fit(x_train, y_train.ravel())\n",
        "\n",
        "    train_predictions = clf.predict(x_train)\n",
        "    train_acc = accuracy_score(y_train, train_predictions)\n",
        "\n",
        "    test_predictions = clf.predict(x_test)\n",
        "    test_acc = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "    return [train_acc, test_acc]\n",
        "\n",
        "def lda_prune_layers_with_adaboost(model, x_train, y_train, x_test, y_test, n_components, margin, n_estimators):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Extract features\n",
        "        x_train_features = model(x_train)\n",
        "        x_test_features = model(x_test)\n",
        "    print(x_train.shape,x_train_features.shape)\n",
        "    # Training and testing of AdaBoost on the new features\n",
        "    [train_acc, test_acc] = ada_boost_classify(x_train_features.numpy(), y_train.numpy(), x_test_features.numpy(), y_test.numpy(), n_estimators)\n",
        "\n",
        "    print(\"Accuracy on train data is:\", train_acc * 100.0)\n",
        "    print(\"Accuracy on test data is:\", test_acc * 100.0)\n",
        "\n",
        "# Usage\n"
      ],
      "metadata": {
        "id": "AH0m8BdxbXaD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WITHOUT LDA"
      ],
      "metadata": {
        "id": "WpNct_qJeqy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Model without LDA\n",
        "model_no_lda = MLP(input_dim=x_train.shape[1], hidden_dim=1024, output_dim=outdim_size)\n",
        "\n",
        "# Training without LDA\n",
        "optimizer_no_lda = optim.Adam(model_no_lda.parameters())\n",
        "criterion_no_lda = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(100):\n",
        "    model_no_lda.train()\n",
        "    optimizer_no_lda.zero_grad()\n",
        "    outputs_no_lda = model_no_lda(torch.tensor(x_train))\n",
        "    loss_no_lda = criterion_no_lda(outputs_no_lda, torch.tensor(y_train))\n",
        "    loss_no_lda.backward()\n",
        "    optimizer_no_lda.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/100], Loss (No LDA): {loss_no_lda.item():.4f}')\n",
        "\n",
        "# Evaluate MLP without LDA\n",
        "model_no_lda.eval()\n",
        "with torch.no_grad():\n",
        "    outputs_train_no_lda = model_no_lda(torch.tensor(x_train))\n",
        "    outputs_test_no_lda = model_no_lda(torch.tensor(x_test))\n",
        "acc_train_no_lda = accuracy_score(y_train, torch.argmax(outputs_train_no_lda, axis=1))\n",
        "acc_test_no_lda = accuracy_score(y_test, torch.argmax(outputs_test_no_lda, axis=1))\n",
        "print(\"Accuracy on train data (No LDA):\", acc_train_no_lda * 100.0)\n",
        "print(\"Accuracy on test data (No LDA):\", acc_test_no_lda * 100.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7mo0bIBcuvV",
        "outputId": "40d05eb9-c2d4-4f2a-df32-74f5f27746e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss (No LDA): 2.3506\n",
            "Epoch [2/100], Loss (No LDA): 2.2475\n",
            "Epoch [3/100], Loss (No LDA): 2.1987\n",
            "Epoch [4/100], Loss (No LDA): 2.1308\n",
            "Epoch [5/100], Loss (No LDA): 2.0538\n",
            "Epoch [6/100], Loss (No LDA): 1.9749\n",
            "Epoch [7/100], Loss (No LDA): 1.8986\n",
            "Epoch [8/100], Loss (No LDA): 1.8347\n",
            "Epoch [9/100], Loss (No LDA): 1.7757\n",
            "Epoch [10/100], Loss (No LDA): 1.7094\n",
            "Epoch [11/100], Loss (No LDA): 1.6379\n",
            "Epoch [12/100], Loss (No LDA): 1.5680\n",
            "Epoch [13/100], Loss (No LDA): 1.5023\n",
            "Epoch [14/100], Loss (No LDA): 1.4405\n",
            "Epoch [15/100], Loss (No LDA): 1.3814\n",
            "Epoch [16/100], Loss (No LDA): 1.3237\n",
            "Epoch [17/100], Loss (No LDA): 1.2662\n",
            "Epoch [18/100], Loss (No LDA): 1.2094\n",
            "Epoch [19/100], Loss (No LDA): 1.1550\n",
            "Epoch [20/100], Loss (No LDA): 1.1043\n",
            "Epoch [21/100], Loss (No LDA): 1.0575\n",
            "Epoch [22/100], Loss (No LDA): 1.0140\n",
            "Epoch [23/100], Loss (No LDA): 0.9728\n",
            "Epoch [24/100], Loss (No LDA): 0.9334\n",
            "Epoch [25/100], Loss (No LDA): 0.8960\n",
            "Epoch [26/100], Loss (No LDA): 0.8609\n",
            "Epoch [27/100], Loss (No LDA): 0.8288\n",
            "Epoch [28/100], Loss (No LDA): 0.7993\n",
            "Epoch [29/100], Loss (No LDA): 0.7721\n",
            "Epoch [30/100], Loss (No LDA): 0.7463\n",
            "Epoch [31/100], Loss (No LDA): 0.7218\n",
            "Epoch [32/100], Loss (No LDA): 0.6988\n",
            "Epoch [33/100], Loss (No LDA): 0.6773\n",
            "Epoch [34/100], Loss (No LDA): 0.6574\n",
            "Epoch [35/100], Loss (No LDA): 0.6389\n",
            "Epoch [36/100], Loss (No LDA): 0.6217\n",
            "Epoch [37/100], Loss (No LDA): 0.6056\n",
            "Epoch [38/100], Loss (No LDA): 0.5903\n",
            "Epoch [39/100], Loss (No LDA): 0.5758\n",
            "Epoch [40/100], Loss (No LDA): 0.5621\n",
            "Epoch [41/100], Loss (No LDA): 0.5493\n",
            "Epoch [42/100], Loss (No LDA): 0.5375\n",
            "Epoch [43/100], Loss (No LDA): 0.5263\n",
            "Epoch [44/100], Loss (No LDA): 0.5159\n",
            "Epoch [45/100], Loss (No LDA): 0.5059\n",
            "Epoch [46/100], Loss (No LDA): 0.4965\n",
            "Epoch [47/100], Loss (No LDA): 0.4876\n",
            "Epoch [48/100], Loss (No LDA): 0.4791\n",
            "Epoch [49/100], Loss (No LDA): 0.4712\n",
            "Epoch [50/100], Loss (No LDA): 0.4637\n",
            "Epoch [51/100], Loss (No LDA): 0.4567\n",
            "Epoch [52/100], Loss (No LDA): 0.4500\n",
            "Epoch [53/100], Loss (No LDA): 0.4436\n",
            "Epoch [54/100], Loss (No LDA): 0.4375\n",
            "Epoch [55/100], Loss (No LDA): 0.4317\n",
            "Epoch [56/100], Loss (No LDA): 0.4262\n",
            "Epoch [57/100], Loss (No LDA): 0.4209\n",
            "Epoch [58/100], Loss (No LDA): 0.4159\n",
            "Epoch [59/100], Loss (No LDA): 0.4111\n",
            "Epoch [60/100], Loss (No LDA): 0.4066\n",
            "Epoch [61/100], Loss (No LDA): 0.4022\n",
            "Epoch [62/100], Loss (No LDA): 0.3980\n",
            "Epoch [63/100], Loss (No LDA): 0.3940\n",
            "Epoch [64/100], Loss (No LDA): 0.3902\n",
            "Epoch [65/100], Loss (No LDA): 0.3865\n",
            "Epoch [66/100], Loss (No LDA): 0.3829\n",
            "Epoch [67/100], Loss (No LDA): 0.3795\n",
            "Epoch [68/100], Loss (No LDA): 0.3763\n",
            "Epoch [69/100], Loss (No LDA): 0.3731\n",
            "Epoch [70/100], Loss (No LDA): 0.3701\n",
            "Epoch [71/100], Loss (No LDA): 0.3672\n",
            "Epoch [72/100], Loss (No LDA): 0.3643\n",
            "Epoch [73/100], Loss (No LDA): 0.3616\n",
            "Epoch [74/100], Loss (No LDA): 0.3590\n",
            "Epoch [75/100], Loss (No LDA): 0.3564\n",
            "Epoch [76/100], Loss (No LDA): 0.3540\n",
            "Epoch [77/100], Loss (No LDA): 0.3516\n",
            "Epoch [78/100], Loss (No LDA): 0.3493\n",
            "Epoch [79/100], Loss (No LDA): 0.3470\n",
            "Epoch [80/100], Loss (No LDA): 0.3448\n",
            "Epoch [81/100], Loss (No LDA): 0.3427\n",
            "Epoch [82/100], Loss (No LDA): 0.3407\n",
            "Epoch [83/100], Loss (No LDA): 0.3387\n",
            "Epoch [84/100], Loss (No LDA): 0.3368\n",
            "Epoch [85/100], Loss (No LDA): 0.3349\n",
            "Epoch [86/100], Loss (No LDA): 0.3330\n",
            "Epoch [87/100], Loss (No LDA): 0.3312\n",
            "Epoch [88/100], Loss (No LDA): 0.3295\n",
            "Epoch [89/100], Loss (No LDA): 0.3278\n",
            "Epoch [90/100], Loss (No LDA): 0.3262\n",
            "Epoch [91/100], Loss (No LDA): 0.3245\n",
            "Epoch [92/100], Loss (No LDA): 0.3230\n",
            "Epoch [93/100], Loss (No LDA): 0.3214\n",
            "Epoch [94/100], Loss (No LDA): 0.3199\n",
            "Epoch [95/100], Loss (No LDA): 0.3185\n",
            "Epoch [96/100], Loss (No LDA): 0.3170\n",
            "Epoch [97/100], Loss (No LDA): 0.3156\n",
            "Epoch [98/100], Loss (No LDA): 0.3143\n",
            "Epoch [99/100], Loss (No LDA): 0.3129\n",
            "Epoch [100/100], Loss (No LDA): 0.3116\n",
            "Accuracy on train data (No LDA): 91.21785714285714\n",
            "Accuracy on test data (No LDA): 91.20714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WITH LDA"
      ],
      "metadata": {
        "id": "BzE6tcU6fp2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_LDA = MLP(input_dim=x_train.shape[1], hidden_dim=1024, output_dim=outdim_size)\n",
        "\n",
        "# Training\n",
        "optimizer_LDA = optim.Adam(model_LDA.parameters())\n",
        "criterion_LDA = LDA_Loss(n_components, margin)\n",
        "for epoch in range(100):\n",
        "    model_LDA.train()\n",
        "    optimizer_LDA.zero_grad()\n",
        "    outputs = model_LDA(torch.tensor(x_train))\n",
        "    loss = criterion_LDA(torch.tensor(y_train), outputs)\n",
        "    loss.backward()\n",
        "    optimizer_LDA.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
        "\n",
        "lda_prune_layers(model_LDA, torch.tensor(x_train), torch.tensor(y_train), torch.tensor(x_test), torch.tensor(y_test), n_components, margin)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP2hClu_fpQU",
        "outputId": "e847a77b-93f3-461f-d3bd-9307b09d6128"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: -0.0001\n",
            "Epoch [2/100], Loss: -0.0028\n",
            "Epoch [3/100], Loss: -0.0086\n",
            "Epoch [4/100], Loss: -0.0188\n",
            "Epoch [5/100], Loss: -0.0351\n",
            "Epoch [6/100], Loss: -0.0594\n",
            "Epoch [7/100], Loss: -0.0944\n",
            "Epoch [8/100], Loss: -0.1430\n",
            "Epoch [9/100], Loss: -0.0678\n",
            "Epoch [10/100], Loss: -0.0933\n",
            "Epoch [11/100], Loss: -0.1290\n",
            "Epoch [12/100], Loss: -0.0285\n",
            "Epoch [13/100], Loss: -0.0362\n",
            "Epoch [14/100], Loss: -0.0470\n",
            "Epoch [15/100], Loss: -0.0619\n",
            "Epoch [16/100], Loss: -0.0823\n",
            "Epoch [17/100], Loss: -0.1101\n",
            "Epoch [18/100], Loss: -0.0047\n",
            "Epoch [19/100], Loss: -0.0055\n",
            "Epoch [20/100], Loss: -0.0065\n",
            "Epoch [21/100], Loss: -0.0078\n",
            "Epoch [22/100], Loss: -0.0095\n",
            "Epoch [23/100], Loss: -0.0116\n",
            "Epoch [24/100], Loss: -0.0143\n",
            "Epoch [25/100], Loss: -0.0178\n",
            "Epoch [26/100], Loss: -0.0223\n",
            "Epoch [27/100], Loss: -0.0281\n",
            "Epoch [28/100], Loss: -0.0357\n",
            "Epoch [29/100], Loss: -0.0456\n",
            "Epoch [30/100], Loss: -0.0586\n",
            "Epoch [31/100], Loss: -0.0758\n",
            "Epoch [32/100], Loss: -0.0985\n",
            "Epoch [33/100], Loss: -0.1287\n",
            "Epoch [34/100], Loss: -0.1686\n",
            "Epoch [35/100], Loss: -0.0276\n",
            "Epoch [36/100], Loss: -0.0332\n",
            "Epoch [37/100], Loss: -0.0406\n",
            "Epoch [38/100], Loss: -0.0504\n",
            "Epoch [39/100], Loss: -0.0634\n",
            "Epoch [40/100], Loss: -0.0807\n",
            "Epoch [41/100], Loss: -0.1037\n",
            "Epoch [42/100], Loss: -0.1345\n",
            "Epoch [43/100], Loss: -0.1757\n",
            "Epoch [44/100], Loss: -0.0114\n",
            "Epoch [45/100], Loss: -0.0133\n",
            "Epoch [46/100], Loss: -0.0158\n",
            "Epoch [47/100], Loss: -0.0190\n",
            "Epoch [48/100], Loss: -0.0233\n",
            "Epoch [49/100], Loss: -0.0288\n",
            "Epoch [50/100], Loss: -0.0362\n",
            "Epoch [51/100], Loss: -0.0460\n",
            "Epoch [52/100], Loss: -0.0591\n",
            "Epoch [53/100], Loss: -0.0768\n",
            "Epoch [54/100], Loss: -0.1009\n",
            "Epoch [55/100], Loss: -0.1337\n",
            "Epoch [56/100], Loss: -0.1787\n",
            "Epoch [57/100], Loss: -0.2401\n",
            "Epoch [58/100], Loss: -0.0026\n",
            "Epoch [59/100], Loss: -0.0029\n",
            "Epoch [60/100], Loss: -0.0033\n",
            "Epoch [61/100], Loss: -0.0038\n",
            "Epoch [62/100], Loss: -0.0043\n",
            "Epoch [63/100], Loss: -0.0050\n",
            "Epoch [64/100], Loss: -0.0059\n",
            "Epoch [65/100], Loss: -0.0069\n",
            "Epoch [66/100], Loss: -0.0082\n",
            "Epoch [67/100], Loss: -0.0098\n",
            "Epoch [68/100], Loss: -0.0118\n",
            "Epoch [69/100], Loss: -0.0142\n",
            "Epoch [70/100], Loss: -0.0173\n",
            "Epoch [71/100], Loss: -0.0213\n",
            "Epoch [72/100], Loss: -0.0264\n",
            "Epoch [73/100], Loss: -0.0329\n",
            "Epoch [74/100], Loss: -0.0415\n",
            "Epoch [75/100], Loss: -0.0527\n",
            "Epoch [76/100], Loss: -0.0675\n",
            "Epoch [77/100], Loss: -0.0873\n",
            "Epoch [78/100], Loss: -0.1138\n",
            "Epoch [79/100], Loss: -0.1494\n",
            "Epoch [80/100], Loss: -0.1974\n",
            "Epoch [81/100], Loss: -0.2621\n",
            "Epoch [82/100], Loss: -0.0005\n",
            "Epoch [83/100], Loss: -0.0005\n",
            "Epoch [84/100], Loss: -0.0006\n",
            "Epoch [85/100], Loss: -0.0006\n",
            "Epoch [86/100], Loss: -0.0007\n",
            "Epoch [87/100], Loss: -0.0008\n",
            "Epoch [88/100], Loss: -0.0009\n",
            "Epoch [89/100], Loss: -0.0010\n",
            "Epoch [90/100], Loss: -0.0011\n",
            "Epoch [91/100], Loss: -0.0013\n",
            "Epoch [92/100], Loss: -0.0014\n",
            "Epoch [93/100], Loss: -0.0016\n",
            "Epoch [94/100], Loss: -0.0018\n",
            "Epoch [95/100], Loss: -0.0021\n",
            "Epoch [96/100], Loss: -0.0024\n",
            "Epoch [97/100], Loss: -0.0028\n",
            "Epoch [98/100], Loss: -0.0032\n",
            "Epoch [99/100], Loss: -0.0037\n",
            "Epoch [100/100], Loss: -0.0042\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 84.25\n",
            "Accuracy on test data is: 84.55714285714285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_prune_layers_with_adaboost(model_LDA, torch.tensor(x_train), torch.tensor(y_train), torch.tensor(x_test), torch.tensor(y_test), n_components, margin, n_estimators=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYh9GynggQEP",
        "outputId": "6015d393-8941-439d-8eac-df9a03dbb5be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training AdaBoost classifier...\n",
            "Accuracy on train data is: 74.37142857142857\n",
            "Accuracy on test data is: 74.12857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WHAT IS IDEAL N_COMPONENTS?"
      ],
      "metadata": {
        "id": "As0JZNZEx0yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_test_acc = 0\n",
        "best_model = None\n",
        "best_n_components = None\n",
        "\n",
        "for n_components in range(1, 10):\n",
        "    model_LDA = MLP(input_dim=x_train.shape[1], hidden_dim=1024, output_dim=outdim_size)\n",
        "\n",
        "    # Training\n",
        "    optimizer_LDA = optim.Adam(model_LDA.parameters())\n",
        "    criterion_LDA = LDA_Loss(n_components, margin)\n",
        "    for epoch in range(100):\n",
        "        model_LDA.train()\n",
        "        optimizer_LDA.zero_grad()\n",
        "        outputs = model_LDA(torch.tensor(x_train))\n",
        "        loss = criterion_LDA(torch.tensor(y_train), outputs)\n",
        "        loss.backward()\n",
        "        optimizer_LDA.step()\n",
        "\n",
        "     #   print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
        "\n",
        "    print(f\"Training for {n_components} components complete.\")\n",
        "\n",
        "    # Evaluate test accuracy\n",
        "    test_acc = lda_prune_layers(model_LDA, torch.tensor(x_train), torch.tensor(y_train), torch.tensor(x_test), torch.tensor(y_test), n_components, margin)\n",
        "\n",
        "    if test_acc > best_test_acc:\n",
        "        best_test_acc = test_acc\n",
        "        best_model = model_LDA\n",
        "        best_n_components = n_components\n",
        "\n",
        "print(f\"Best test accuracy: {best_test_acc} with {best_n_components} components.\")\n",
        "# Save the best model if needed\n",
        "# torch.save(best_model.state_dict(), \"best_lda_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQxm4B3kx0a7",
        "outputId": "ca031798-b8b8-4db0-c847-4cabbf761976"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 1 components complete.\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 35.949999999999996\n",
            "Accuracy on test data is: 36.22142857142857\n",
            "Training for 2 components complete.\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 57.6125\n",
            "Accuracy on test data is: 58.76428571428571\n",
            "Training for 3 components complete.\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 70.65535714285714\n",
            "Accuracy on test data is: 71.02142857142857\n",
            "Training for 4 components complete.\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 72.85357142857143\n",
            "Accuracy on test data is: 73.65714285714286\n",
            "Training for 5 components complete.\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 79.00178571428572\n",
            "Accuracy on test data is: 79.03571428571429\n",
            "Training for 6 components complete.\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 80.54464285714286\n",
            "Accuracy on test data is: 80.49285714285715\n",
            "Training for 7 components complete.\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 81.88392857142858\n",
            "Accuracy on test data is: 82.1\n",
            "Training for 8 components complete.\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 83.76249999999999\n",
            "Accuracy on test data is: 83.78571428571429\n",
            "Training for 9 components complete.\n",
            "torch.Size([56000, 784]) torch.Size([56000, 10])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 84.22678571428571\n",
            "Accuracy on test data is: 84.43571428571428\n",
            "Best test accuracy: 0.8443571428571428 with 9 components.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BINARY CLASSIFICATION"
      ],
      "metadata": {
        "id": "2mnKCR3_yoBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "-immFJAWgpGZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parameters\n",
        "outdim_size = 2  # 2 classes\n",
        "n_components = 1  # We're using LDA for binary classification\n",
        "margin = 1.0\n",
        "n_estimators = 50  # Number of estimators for AdaBoost"
      ],
      "metadata": {
        "id": "O_1kSZc4gtnQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert input data to torch.float64\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float64)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float64)\n",
        "\n",
        "# Model without LDA\n",
        "model_no_lda = MLP(input_dim=x_train_tensor.shape[1], hidden_dim=1024, output_dim=outdim_size)\n",
        "model_no_lda.double()  # Set model parameters to double precision\n",
        "\n",
        "# Training without LDA\n",
        "optimizer_no_lda = optim.Adam(model_no_lda.parameters())\n",
        "criterion_no_lda = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(100):\n",
        "    model_no_lda.train()\n",
        "    optimizer_no_lda.zero_grad()\n",
        "    outputs_no_lda = model_no_lda(x_train_tensor)  # Use converted tensor\n",
        "    loss_no_lda = criterion_no_lda(outputs_no_lda, torch.tensor(y_train))\n",
        "    loss_no_lda.backward()\n",
        "    optimizer_no_lda.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/100], Loss (No LDA): {loss_no_lda.item():.4f}')\n",
        "\n",
        "# Evaluate MLP without LDA\n",
        "model_no_lda.eval()\n",
        "with torch.no_grad():\n",
        "    outputs_train_no_lda = model_no_lda(x_train_tensor)  # Use converted tensor\n",
        "    outputs_test_no_lda = model_no_lda(x_test_tensor)  # Use converted tensor\n",
        "acc_train_no_lda = accuracy_score(y_train, torch.argmax(outputs_train_no_lda, axis=1))\n",
        "acc_test_no_lda = accuracy_score(y_test, torch.argmax(outputs_test_no_lda, axis=1))\n",
        "print(\"Accuracy on train data (No LDA):\", acc_train_no_lda * 100.0)\n",
        "print(\"Accuracy on test data (No LDA):\", acc_test_no_lda * 100.0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZxlBfSyg_Dk",
        "outputId": "53babc97-ad12-480f-b07b-4c7c90bef787"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss (No LDA): 0.6873\n",
            "Epoch [2/100], Loss (No LDA): 0.6770\n",
            "Epoch [3/100], Loss (No LDA): 0.6160\n",
            "Epoch [4/100], Loss (No LDA): 0.5698\n",
            "Epoch [5/100], Loss (No LDA): 0.5574\n",
            "Epoch [6/100], Loss (No LDA): 0.5279\n",
            "Epoch [7/100], Loss (No LDA): 0.4888\n",
            "Epoch [8/100], Loss (No LDA): 0.4646\n",
            "Epoch [9/100], Loss (No LDA): 0.4509\n",
            "Epoch [10/100], Loss (No LDA): 0.4329\n",
            "Epoch [11/100], Loss (No LDA): 0.4097\n",
            "Epoch [12/100], Loss (No LDA): 0.3892\n",
            "Epoch [13/100], Loss (No LDA): 0.3752\n",
            "Epoch [14/100], Loss (No LDA): 0.3626\n",
            "Epoch [15/100], Loss (No LDA): 0.3477\n",
            "Epoch [16/100], Loss (No LDA): 0.3322\n",
            "Epoch [17/100], Loss (No LDA): 0.3199\n",
            "Epoch [18/100], Loss (No LDA): 0.3111\n",
            "Epoch [19/100], Loss (No LDA): 0.3027\n",
            "Epoch [20/100], Loss (No LDA): 0.2932\n",
            "Epoch [21/100], Loss (No LDA): 0.2834\n",
            "Epoch [22/100], Loss (No LDA): 0.2757\n",
            "Epoch [23/100], Loss (No LDA): 0.2695\n",
            "Epoch [24/100], Loss (No LDA): 0.2627\n",
            "Epoch [25/100], Loss (No LDA): 0.2552\n",
            "Epoch [26/100], Loss (No LDA): 0.2483\n",
            "Epoch [27/100], Loss (No LDA): 0.2433\n",
            "Epoch [28/100], Loss (No LDA): 0.2397\n",
            "Epoch [29/100], Loss (No LDA): 0.2359\n",
            "Epoch [30/100], Loss (No LDA): 0.2323\n",
            "Epoch [31/100], Loss (No LDA): 0.2289\n",
            "Epoch [32/100], Loss (No LDA): 0.2265\n",
            "Epoch [33/100], Loss (No LDA): 0.2242\n",
            "Epoch [34/100], Loss (No LDA): 0.2215\n",
            "Epoch [35/100], Loss (No LDA): 0.2186\n",
            "Epoch [36/100], Loss (No LDA): 0.2160\n",
            "Epoch [37/100], Loss (No LDA): 0.2130\n",
            "Epoch [38/100], Loss (No LDA): 0.2097\n",
            "Epoch [39/100], Loss (No LDA): 0.2069\n",
            "Epoch [40/100], Loss (No LDA): 0.2049\n",
            "Epoch [41/100], Loss (No LDA): 0.2034\n",
            "Epoch [42/100], Loss (No LDA): 0.2019\n",
            "Epoch [43/100], Loss (No LDA): 0.2004\n",
            "Epoch [44/100], Loss (No LDA): 0.1988\n",
            "Epoch [45/100], Loss (No LDA): 0.1972\n",
            "Epoch [46/100], Loss (No LDA): 0.1958\n",
            "Epoch [47/100], Loss (No LDA): 0.1941\n",
            "Epoch [48/100], Loss (No LDA): 0.1921\n",
            "Epoch [49/100], Loss (No LDA): 0.1904\n",
            "Epoch [50/100], Loss (No LDA): 0.1893\n",
            "Epoch [51/100], Loss (No LDA): 0.1882\n",
            "Epoch [52/100], Loss (No LDA): 0.1872\n",
            "Epoch [53/100], Loss (No LDA): 0.1862\n",
            "Epoch [54/100], Loss (No LDA): 0.1848\n",
            "Epoch [55/100], Loss (No LDA): 0.1838\n",
            "Epoch [56/100], Loss (No LDA): 0.1828\n",
            "Epoch [57/100], Loss (No LDA): 0.1817\n",
            "Epoch [58/100], Loss (No LDA): 0.1807\n",
            "Epoch [59/100], Loss (No LDA): 0.1798\n",
            "Epoch [60/100], Loss (No LDA): 0.1789\n",
            "Epoch [61/100], Loss (No LDA): 0.1781\n",
            "Epoch [62/100], Loss (No LDA): 0.1772\n",
            "Epoch [63/100], Loss (No LDA): 0.1762\n",
            "Epoch [64/100], Loss (No LDA): 0.1753\n",
            "Epoch [65/100], Loss (No LDA): 0.1744\n",
            "Epoch [66/100], Loss (No LDA): 0.1736\n",
            "Epoch [67/100], Loss (No LDA): 0.1729\n",
            "Epoch [68/100], Loss (No LDA): 0.1722\n",
            "Epoch [69/100], Loss (No LDA): 0.1714\n",
            "Epoch [70/100], Loss (No LDA): 0.1706\n",
            "Epoch [71/100], Loss (No LDA): 0.1698\n",
            "Epoch [72/100], Loss (No LDA): 0.1690\n",
            "Epoch [73/100], Loss (No LDA): 0.1683\n",
            "Epoch [74/100], Loss (No LDA): 0.1675\n",
            "Epoch [75/100], Loss (No LDA): 0.1668\n",
            "Epoch [76/100], Loss (No LDA): 0.1661\n",
            "Epoch [77/100], Loss (No LDA): 0.1654\n",
            "Epoch [78/100], Loss (No LDA): 0.1647\n",
            "Epoch [79/100], Loss (No LDA): 0.1640\n",
            "Epoch [80/100], Loss (No LDA): 0.1633\n",
            "Epoch [81/100], Loss (No LDA): 0.1626\n",
            "Epoch [82/100], Loss (No LDA): 0.1619\n",
            "Epoch [83/100], Loss (No LDA): 0.1613\n",
            "Epoch [84/100], Loss (No LDA): 0.1606\n",
            "Epoch [85/100], Loss (No LDA): 0.1599\n",
            "Epoch [86/100], Loss (No LDA): 0.1592\n",
            "Epoch [87/100], Loss (No LDA): 0.1586\n",
            "Epoch [88/100], Loss (No LDA): 0.1579\n",
            "Epoch [89/100], Loss (No LDA): 0.1572\n",
            "Epoch [90/100], Loss (No LDA): 0.1566\n",
            "Epoch [91/100], Loss (No LDA): 0.1559\n",
            "Epoch [92/100], Loss (No LDA): 0.1552\n",
            "Epoch [93/100], Loss (No LDA): 0.1545\n",
            "Epoch [94/100], Loss (No LDA): 0.1538\n",
            "Epoch [95/100], Loss (No LDA): 0.1531\n",
            "Epoch [96/100], Loss (No LDA): 0.1525\n",
            "Epoch [97/100], Loss (No LDA): 0.1518\n",
            "Epoch [98/100], Loss (No LDA): 0.1511\n",
            "Epoch [99/100], Loss (No LDA): 0.1505\n",
            "Epoch [100/100], Loss (No LDA): 0.1498\n",
            "Accuracy on train data (No LDA): 93.84615384615384\n",
            "Accuracy on test data (No LDA): 95.6140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with LDA\n",
        "model_lda = MLP(input_dim=x_train_tensor.shape[1], hidden_dim=1024, output_dim=outdim_size)\n",
        "model_lda.double()  # Set model parameters to double precision\n",
        "\n",
        "# Training with LDA\n",
        "optimizer_lda = optim.Adam(model_lda.parameters())\n",
        "criterion_lda = LDA_Loss(n_components, margin)\n",
        "\n",
        "for epoch in range(100):\n",
        "    model_lda.train()\n",
        "    optimizer_lda.zero_grad()\n",
        "    outputs_lda = model_lda(torch.tensor(x_train, dtype=torch.float64))  # Use converted tensor\n",
        "    loss_lda = criterion_lda(torch.tensor(y_train), outputs_lda)\n",
        "    loss_lda.backward()\n",
        "    optimizer_lda.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/100], Loss (With LDA): {loss_lda.item():.4f}')\n",
        "\n",
        "# Prune Layers and SVM Classification with LDA\n",
        "lda_prune_layers(model_lda, torch.tensor(x_train, dtype=torch.float64), torch.tensor(y_train), torch.tensor(x_test, dtype=torch.float64), torch.tensor(y_test), n_components, margin)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w66q40yJhSmg",
        "outputId": "c1605551-5d5b-44ad-8397-bd1ac6049dfc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss (With LDA): -0.0014\n",
            "Epoch [2/100], Loss (With LDA): -0.0114\n",
            "Epoch [3/100], Loss (With LDA): -0.0287\n",
            "Epoch [4/100], Loss (With LDA): -0.0528\n",
            "Epoch [5/100], Loss (With LDA): -0.0841\n",
            "Epoch [6/100], Loss (With LDA): -0.1239\n",
            "Epoch [7/100], Loss (With LDA): -0.1727\n",
            "Epoch [8/100], Loss (With LDA): -0.2315\n",
            "Epoch [9/100], Loss (With LDA): -0.2970\n",
            "Epoch [10/100], Loss (With LDA): -0.3695\n",
            "Epoch [11/100], Loss (With LDA): -0.4528\n",
            "Epoch [12/100], Loss (With LDA): -0.5463\n",
            "Epoch [13/100], Loss (With LDA): -0.6522\n",
            "Epoch [14/100], Loss (With LDA): -0.7742\n",
            "Epoch [15/100], Loss (With LDA): -0.9133\n",
            "Epoch [16/100], Loss (With LDA): -1.0720\n",
            "Epoch [17/100], Loss (With LDA): -1.2482\n",
            "Epoch [18/100], Loss (With LDA): -1.4384\n",
            "Epoch [19/100], Loss (With LDA): -1.6439\n",
            "Epoch [20/100], Loss (With LDA): -1.8672\n",
            "Epoch [21/100], Loss (With LDA): -2.1154\n",
            "Epoch [22/100], Loss (With LDA): -2.4016\n",
            "Epoch [23/100], Loss (With LDA): -2.7270\n",
            "Epoch [24/100], Loss (With LDA): -3.1016\n",
            "Epoch [25/100], Loss (With LDA): -3.5164\n",
            "Epoch [26/100], Loss (With LDA): -3.9766\n",
            "Epoch [27/100], Loss (With LDA): -4.4778\n",
            "Epoch [28/100], Loss (With LDA): -5.0425\n",
            "Epoch [29/100], Loss (With LDA): -5.6685\n",
            "Epoch [30/100], Loss (With LDA): -6.3479\n",
            "Epoch [31/100], Loss (With LDA): -7.0941\n",
            "Epoch [32/100], Loss (With LDA): -7.9165\n",
            "Epoch [33/100], Loss (With LDA): -8.8271\n",
            "Epoch [34/100], Loss (With LDA): -9.8518\n",
            "Epoch [35/100], Loss (With LDA): -10.9609\n",
            "Epoch [36/100], Loss (With LDA): -12.1315\n",
            "Epoch [37/100], Loss (With LDA): -13.3246\n",
            "Epoch [38/100], Loss (With LDA): -14.5812\n",
            "Epoch [39/100], Loss (With LDA): -15.9540\n",
            "Epoch [40/100], Loss (With LDA): -17.4112\n",
            "Epoch [41/100], Loss (With LDA): -18.9588\n",
            "Epoch [42/100], Loss (With LDA): -20.6185\n",
            "Epoch [43/100], Loss (With LDA): -22.3867\n",
            "Epoch [44/100], Loss (With LDA): -24.2616\n",
            "Epoch [45/100], Loss (With LDA): -26.3101\n",
            "Epoch [46/100], Loss (With LDA): -28.4981\n",
            "Epoch [47/100], Loss (With LDA): -30.7262\n",
            "Epoch [48/100], Loss (With LDA): -33.0139\n",
            "Epoch [49/100], Loss (With LDA): -35.4376\n",
            "Epoch [50/100], Loss (With LDA): -37.9887\n",
            "Epoch [51/100], Loss (With LDA): -40.6735\n",
            "Epoch [52/100], Loss (With LDA): -43.5971\n",
            "Epoch [53/100], Loss (With LDA): -46.5856\n",
            "Epoch [54/100], Loss (With LDA): -49.6598\n",
            "Epoch [55/100], Loss (With LDA): -53.0239\n",
            "Epoch [56/100], Loss (With LDA): -56.6437\n",
            "Epoch [57/100], Loss (With LDA): -60.3854\n",
            "Epoch [58/100], Loss (With LDA): -64.3189\n",
            "Epoch [59/100], Loss (With LDA): -68.4723\n",
            "Epoch [60/100], Loss (With LDA): -72.8233\n",
            "Epoch [61/100], Loss (With LDA): -77.3185\n",
            "Epoch [62/100], Loss (With LDA): -82.0841\n",
            "Epoch [63/100], Loss (With LDA): -87.0904\n",
            "Epoch [64/100], Loss (With LDA): -92.2305\n",
            "Epoch [65/100], Loss (With LDA): -97.5045\n",
            "Epoch [66/100], Loss (With LDA): -102.9668\n",
            "Epoch [67/100], Loss (With LDA): -108.6710\n",
            "Epoch [68/100], Loss (With LDA): -114.7813\n",
            "Epoch [69/100], Loss (With LDA): -121.5111\n",
            "Epoch [70/100], Loss (With LDA): -128.7412\n",
            "Epoch [71/100], Loss (With LDA): -136.1512\n",
            "Epoch [72/100], Loss (With LDA): -143.3246\n",
            "Epoch [73/100], Loss (With LDA): -149.9761\n",
            "Epoch [74/100], Loss (With LDA): -156.7928\n",
            "Epoch [75/100], Loss (With LDA): -164.1014\n",
            "Epoch [76/100], Loss (With LDA): -171.8982\n",
            "Epoch [77/100], Loss (With LDA): -180.2433\n",
            "Epoch [78/100], Loss (With LDA): -189.1154\n",
            "Epoch [79/100], Loss (With LDA): -198.2610\n",
            "Epoch [80/100], Loss (With LDA): -207.4522\n",
            "Epoch [81/100], Loss (With LDA): -216.5741\n",
            "Epoch [82/100], Loss (With LDA): -226.0766\n",
            "Epoch [83/100], Loss (With LDA): -236.0073\n",
            "Epoch [84/100], Loss (With LDA): -246.3924\n",
            "Epoch [85/100], Loss (With LDA): -257.1323\n",
            "Epoch [86/100], Loss (With LDA): -268.0381\n",
            "Epoch [87/100], Loss (With LDA): -279.3535\n",
            "Epoch [88/100], Loss (With LDA): -291.0787\n",
            "Epoch [89/100], Loss (With LDA): -302.9983\n",
            "Epoch [90/100], Loss (With LDA): -314.9087\n",
            "Epoch [91/100], Loss (With LDA): -326.8793\n",
            "Epoch [92/100], Loss (With LDA): -339.2314\n",
            "Epoch [93/100], Loss (With LDA): -351.9578\n",
            "Epoch [94/100], Loss (With LDA): -365.4506\n",
            "Epoch [95/100], Loss (With LDA): -379.5741\n",
            "Epoch [96/100], Loss (With LDA): -393.3852\n",
            "Epoch [97/100], Loss (With LDA): -407.4051\n",
            "Epoch [98/100], Loss (With LDA): -421.9750\n",
            "Epoch [99/100], Loss (With LDA): -436.9112\n",
            "Epoch [100/100], Loss (With LDA): -452.0391\n",
            "torch.Size([455, 30]) torch.Size([455, 2])\n",
            "Training SVM...\n",
            "Accuracy on train data is: 91.20879120879121\n",
            "Accuracy on test data is: 92.98245614035088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda_prune_layers_with_adaboost(model_lda, torch.tensor(x_train, dtype=torch.float64), torch.tensor(y_train), torch.tensor(x_test, dtype=torch.float64), torch.tensor(y_test),n_components, margin, n_estimators=100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5opH79Sh4Z9",
        "outputId": "257502ba-2f9b-4da2-b987-23c93b778628"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([455, 30]) torch.Size([455, 2])\n",
            "Training AdaBoost classifier...\n",
            "Accuracy on train data is: 95.16483516483515\n",
            "Accuracy on test data is: 92.10526315789474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dpzQ-P-gtY3q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}