{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 12:53:39.211333: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-24 12:53:39.275955: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-24 12:53:40.252070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from models import *\n",
    "from collections import namedtuple\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "import tokenization\n",
    "import models\n",
    "import optim as optim\n",
    "import train_org as train\n",
    "from utils import set_seeds, get_device, truncate_tokens_pair\n",
    "from classify import dataset_class, Tokenizing, AddSpecialTokensWithTruncation, TokenIndexing, Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Const Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Const Values for the file locations\n",
    "task='mrpc'\n",
    "\n",
    "train_cfg= f'config/train_{task}.json'\n",
    "model_cfg='config/bert_base.json'\n",
    "\n",
    "train_data_file = f'/home/common_algorithm/dataset/GLUE/{task.upper()}/train.tsv'\n",
    "test_data_file=f'/home/common_algorithm/dataset/GLUE/{task.upper()}/dev.tsv'\n",
    "\n",
    "pretrain_file = './PRE_TRAINED_MODEL/bert_model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Const values for the network \n",
    "max_len=128\n",
    "\n",
    "#vocab='./PRE_TRAINED_MODEL/vocab.txt'\n",
    "vocab = '/home/common_algorithm/checkpoints/GLUE/PRE_TRAINED_MODEL/vocab.txt'\n",
    "save_dir = './SAVE'\n",
    "model_file = '/home/prbhatnagar/Desktop/Final_Project (copy)/finetuned/finetuned_mrpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = train.Config.from_json(train_cfg)\n",
    "model_cfg = models.Config.from_json(model_cfg)\n",
    "set_seeds(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(vocab_size=30522, dim=768, n_layers=12, n_heads=12, dim_ff=3072, p_drop_hidden=0.1, p_drop_attn=0.1, max_len=512, n_segments=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab, do_lower_case=True)\n",
    "TaskDataset = dataset_class(task) # task dataset class according to the task\n",
    "pipeline = [Tokenizing(tokenizer.convert_to_unicode, tokenizer.tokenize),\n",
    "            AddSpecialTokensWithTruncation(max_len),\n",
    "            TokenIndexing(tokenizer.convert_tokens_to_ids,\n",
    "                            TaskDataset.labels, max_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaskDataset(train_data_file, pipeline)\n",
    "test_dataset = TaskDataset(test_data_file, pipeline)\n",
    "train_data_iter = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "test_data_iter = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batch):\n",
    "    input_ids, segment_ids, input_mask, label_id = batch\n",
    "    logits = model(input_ids, segment_ids, input_mask)\n",
    "    _, label_pred = logits.max(1)\n",
    "    result = (label_pred == label_id).float() #.cpu().numpy()\n",
    "    accuracy = result.mean()\n",
    "    return accuracy, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, batch, global_step): # make sure loss is a scalar tensor\n",
    "    input_ids, segment_ids, input_mask, label_id = batch\n",
    "    logits = model(input_ids, segment_ids, input_mask)\n",
    "    loss = criterion(logits, label_id)\n",
    "    return loss\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (2 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "trainer = train.Trainer(cfg, model, train_data_iter, test_data_iter, optim.optim4GPU(cfg, model),save_dir, get_device())\n",
    "trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (2 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomp_model = Classifier(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "decomp_trainer = train.Trainer(cfg, decomp_model, train_data_iter, test_data_iter, optim.optim4GPU(cfg, decomp_model),save_dir, get_device())\n",
    "decomp_trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter(acc=1.000): 100%|████████████████████████| 102/102 [00:03<00:00, 30.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867647111415863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = trainer.eval(evaluate)\n",
    "total_accuracy = torch.cat(results).mean().item()\n",
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"Implementation of the gelu activation function by Hugging Face\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCCompression(nn.Module):\n",
    "    def __init__(self, cfg, k):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.fc1_u = nn.Linear(768, k, bias = False)\n",
    "        self.fc1_vs = nn.Linear(k, 3072)\n",
    "        \n",
    "        self.fc2_u = nn.Linear(3072, k)\n",
    "        self.fc2_vs = nn.Linear(k, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = gelu(self.fc1_vs(self.fc1_u(x)))\n",
    "        out = self.fc2_vs(self.fc2_u(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_matrix(mat, k):\n",
    "    U, s, VT = np.linalg.svd(mat, full_matrices=False)\n",
    "    Sigma = np.diag(s[:k])\n",
    "    U_truncated = U[:, :k]\n",
    "    VT_truncated = VT[:k, :]\n",
    "    return U_truncated, Sigma, VT_truncated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomp_model_func(decomp_trainer, rank):\n",
    "    for b in range(len(trainer.model.transformer.blocks)):\n",
    "        \n",
    "        block = trainer.model.transformer.blocks[b]\n",
    "        target_block = decomp_trainer.model.transformer.blocks[b]\n",
    "\n",
    "        target_block.pwff = FCCompression(model_cfg, rank[b])\n",
    "        \n",
    "        fc1_U, fc1_Sigma, fc1_V = decompose_matrix(block.pwff.fc1.weight.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.pwff.fc1_u.weight.data = torch.from_numpy(fc1_U).T.cuda().contiguous()\n",
    "        target_block.pwff.fc1_vs.weight.data = torch.from_numpy(fc1_Sigma@fc1_V).T.cuda().contiguous()\n",
    "        target_block.pwff.fc1_vs.bias.data = block.pwff.fc1.bias\n",
    "        \n",
    "        fc2_U, fc2_Sigma, fc2_V = decompose_matrix(block.pwff.fc2.weight.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.pwff.fc2_u.weight.data = torch.from_numpy(fc2_U).T.cuda().contiguous()\n",
    "        target_block.pwff.fc2_vs.weight.data = torch.from_numpy(fc2_Sigma@fc2_V).T.cuda().contiguous()\n",
    "        target_block.pwff.fc2_vs.bias.data = block.pwff.fc2.bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [400]*12\n",
    "decomp_model_func(decomp_trainer, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34895833333333337\n"
     ]
    }
   ],
   "source": [
    "compression = 0\n",
    "for r in rank:\n",
    "    compression += (3072*r + 768*r)\n",
    "\n",
    "compression = 1 - compression/ (12*768*3072)\n",
    "print(compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter(acc=0.750): 100%|████████████████████████| 102/102 [00:02<00:00, 42.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7524510025978088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model performance after FC Compression\n",
    "results = decomp_trainer.eval(evaluate)\n",
    "total_accuracy = torch.cat(results).mean().item()\n",
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadProjection(nn.Module):\n",
    "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
    "    def __init__(self, cfg, k):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
    "        \n",
    "        self.proj_q_u = nn.Linear(768,k)\n",
    "        self.proj_q_vs = nn.Linear(k, 768, bias = False)\n",
    "        \n",
    "        self.proj_k_u = nn.Linear(768,k)\n",
    "        self.proj_k_vs = nn.Linear(k, 768, bias = False)\n",
    "        \n",
    "        self.proj_v_u = nn.Linear(768,k)\n",
    "        self.proj_v_vs = nn.Linear(k, 768, bias = False)\n",
    "            \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        \"\"\"\n",
    "    \n",
    "        q = self.proj_q_u(self.proj_q_vs(x))\n",
    "        k = self.proj_k_u(self.proj_k_vs(x))\n",
    "        v = self.proj_v_u(self.proj_v_vs(x))\n",
    "        \n",
    "        q, k, v = (split_last(x, (12, -1)).transpose(1, 2) for x in [q, k, v])\n",
    "        \n",
    "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :].float()\n",
    "            scores -= 10000.0 * (1.0 - mask)\n",
    "\n",
    "        scores = self.drop(F.softmax(scores, dim=-1))\n",
    "        h = (scores @ v).transpose(1, 2).contiguous()\n",
    "        h = merge_last(h, 2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomp_proj(decom_trainer, rank):\n",
    "    for b in range(len(trainer.model.transformer.blocks)):\n",
    "        \n",
    "        block = trainer.model.transformer.blocks[b]\n",
    "        target_block = decomp_trainer.model.transformer.blocks[b]\n",
    "\n",
    "        target_block.attn = MultiHeadProjection(model_cfg, rank[b]) #, thres = decomp_trainer.model.thres\n",
    "        \n",
    "        proj_q_u, proj_q_s, proj_q_v = decompose_matrix(block.attn.proj_q.weight.T.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.attn.proj_q_u.weight.data = torch.from_numpy(proj_q_u).cuda().contiguous()\n",
    "        target_block.attn.proj_q_vs.weight.data = torch.from_numpy(proj_q_s@proj_q_v).cuda().contiguous()\n",
    "        target_block.attn.proj_q_u.bias.data = block.attn.proj_q.bias\n",
    "        \n",
    "        proj_k_u, proj_k_s, proj_k_v = decompose_matrix(block.attn.proj_k.weight.T.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.attn.proj_k_u.weight.data = torch.from_numpy(proj_k_u).cuda().contiguous()\n",
    "        target_block.attn.proj_k_vs.weight.data = torch.from_numpy(proj_k_s@proj_k_v).cuda().contiguous()\n",
    "        target_block.attn.proj_k_u.bias.data = block.attn.proj_k.bias\n",
    "        \n",
    "        proj_v_u, proj_v_s, proj_v_v = decompose_matrix(block.attn.proj_v.weight.T.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.attn.proj_v_u.weight.data = torch.from_numpy(proj_v_u).cuda().contiguous()\n",
    "        target_block.attn.proj_v_vs.weight.data = torch.from_numpy(proj_v_s@proj_v_v).cuda().contiguous()\n",
    "        target_block.attn.proj_v_u.bias.data = block.attn.proj_v.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [300]*12\n",
    "decomp_proj(decomp_trainer, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21875\n"
     ]
    }
   ],
   "source": [
    "cr = [(2*x*768) / (768*768) for x in rank]\n",
    "print(1 - np.mean(cr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter(acc=0.750): 100%|████████████████████████| 102/102 [00:02<00:00, 37.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7034313678741455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model performance after Attn Compression\n",
    "results = decomp_trainer.eval(evaluate)\n",
    "total_accuracy = torch.cat(results).mean().item()\n",
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadProjection(\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (proj_q_u): Linear(in_features=768, out_features=300, bias=True)\n",
       "  (proj_q_vs): Linear(in_features=300, out_features=768, bias=False)\n",
       "  (proj_k_u): Linear(in_features=768, out_features=300, bias=True)\n",
       "  (proj_k_vs): Linear(in_features=300, out_features=768, bias=False)\n",
       "  (proj_v_u): Linear(in_features=768, out_features=300, bias=True)\n",
       "  (proj_v_vs): Linear(in_features=300, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomp_trainer.model.transformer.blocks[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=0.057): 100%|██████████████████████| 230/230 [00:34<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 : Average Loss 0.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=0.007): 100%|██████████████████████| 230/230 [00:31<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 : Average Loss 0.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=0.003): 100%|██████████████████████| 230/230 [00:30<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 : Average Loss 0.101\n"
     ]
    }
   ],
   "source": [
    "decomp_trainer.train(get_loss, n_epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter(acc=0.750): 100%|████████████████████████| 102/102 [00:02<00:00, 36.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843137264251709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model performance after Finetuning\n",
    "results = decomp_trainer.eval(evaluate)\n",
    "total_accuracy = torch.cat(results).mean().item()\n",
    "print(total_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
