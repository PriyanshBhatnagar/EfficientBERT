{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 23:40:21.824402: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from models import *\n",
    "from collections import namedtuple\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.utils.prune as prune\n",
    "import tokenization\n",
    "import models\n",
    "import optim as optim\n",
    "import train_org as train\n",
    "from utils import set_seeds, get_device, truncate_tokens_pair\n",
    "from classify import dataset_class, Tokenizing, AddSpecialTokensWithTruncation, TokenIndexing, Classifier\n",
    "from classify_SVM import dataset_class, Tokenizing, AddSpecialTokensWithTruncation, TokenIndexing, Classifier_feats\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Const Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Const Values for the file locations\n",
    "task='mrpc'\n",
    "\n",
    "train_cfg= f'config/train_{task}.json'\n",
    "model_cfg='config/bert_base.json'\n",
    "\n",
    "train_data_file = './train.tsv'\n",
    "test_data_file='./dev.tsv'\n",
    "\n",
    "pretrain_file = './bert_model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Const values for the network \n",
    "max_len=128\n",
    "\n",
    "#vocab='./PRE_TRAINED_MODEL/vocab.txt'\n",
    "vocab = './vocab.txt'\n",
    "save_dir = './'\n",
    "model_file = './finetuned_mrpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = train.Config.from_json(train_cfg)\n",
    "model_cfg = models.Config.from_json(model_cfg)\n",
    "set_seeds(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(vocab_size=30522, dim=768, n_layers=12, n_heads=12, dim_ff=3072, p_drop_hidden=0.1, p_drop_attn=0.1, max_len=512, n_segments=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab, do_lower_case=True)\n",
    "TaskDataset = dataset_class(task) # task dataset class according to the task\n",
    "pipeline = [Tokenizing(tokenizer.convert_to_unicode, tokenizer.tokenize),\n",
    "            AddSpecialTokensWithTruncation(max_len),\n",
    "            TokenIndexing(tokenizer.convert_tokens_to_ids,\n",
    "                            TaskDataset.labels, max_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaskDataset(train_data_file, pipeline)\n",
    "test_dataset = TaskDataset(test_data_file, pipeline)\n",
    "train_data_iter = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "test_data_iter = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 1\n",
    "margin = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batch):\n",
    "    input_ids, segment_ids, input_mask, label_id = batch\n",
    "    logits = model(input_ids, segment_ids, input_mask)\n",
    "    _, label_pred = logits.max(1)\n",
    "    result = (label_pred == label_id).float() #.cpu().numpy()\n",
    "    accuracy = result.mean()\n",
    "    return accuracy, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, batch, global_step): # make sure loss is a scalar tensor\n",
    "    model.cuda()\n",
    "    input_ids, segment_ids, input_mask, label_id = batch\n",
    "    logits = model(input_ids, segment_ids, input_mask)\n",
    "    loss_function = LDA_Loss(n_components, margin)\n",
    "    loss = loss_function(label_id, logits)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class LDA_Loss(nn.Module):\n",
    "    def __init__(self, n_components, margin):\n",
    "        super(LDA_Loss, self).__init__()\n",
    "        self.n_components = n_components\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        r = 1e-4\n",
    "\n",
    "        # Initialize groups\n",
    "        groups = torch.unique(y_true)\n",
    "      #  print(y_pred.shape)\n",
    "        def compute_cov(group, Xt, yt):\n",
    "            Xgt = Xt[yt == group]\n",
    "            Xgt_bar = Xgt - torch.mean(Xgt, axis=0)\n",
    "            m = float(Xgt_bar.shape[0])\n",
    "            if m > 1:\n",
    "                return (1.0 / (m - 1)) * torch.matmul(Xgt_bar.T, Xgt_bar)\n",
    "            else:\n",
    "                # If there's only one sample or no sample for this group,\n",
    "                # return a zero matrix of the appropriate size\n",
    "                return torch.zeros_like(torch.matmul(Xgt_bar.T, Xgt_bar))\n",
    "\n",
    "        # Scan over groups\n",
    "        covs_t = torch.stack([compute_cov(group, y_pred, y_true) for group in groups])\n",
    "\n",
    "        # Compute average covariance matrix (within scatter)\n",
    "        Sw_t = torch.mean(covs_t, axis=0)\n",
    "\n",
    "        # Compute total scatter\n",
    "        Xt_bar = y_pred - torch.mean(y_pred, axis=0)\n",
    "        m = float(Xt_bar.shape[0])\n",
    "        St_t = (1.0 / (m - 1)) * torch.matmul(Xt_bar.T, Xt_bar)\n",
    "\n",
    "        # Compute between scatter\n",
    "        Sb_t = St_t - Sw_t\n",
    "\n",
    "        # Cope for numerical instability (regularize)\n",
    "        Sw_t += torch.eye(Sw_t.shape[0], device=Sw_t.device) * r\n",
    "\n",
    "        # Compute eigenvalues\n",
    "        evals_t = torch.linalg.eigvalsh(Sb_t, UPLO='U')  # Use UPLO='U' for upper triangular portion\n",
    "\n",
    "        # Get top eigenvalues\n",
    "        top_k_evals = evals_t[-self.n_components:]\n",
    "\n",
    "        # Maximize variance between classes\n",
    "        thresh = torch.min(top_k_evals) + self.margin\n",
    "        top_k_evals = top_k_evals[top_k_evals <= thresh]\n",
    "        costs = torch.mean(top_k_evals)\n",
    "\n",
    "        return -costs\n",
    "\n",
    "\n",
    "criterion = LDA_Loss(n_components, margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (1 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier_feats(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "trainer = train.Trainer(cfg, model, train_data_iter, test_data_iter, optim.optim4GPU(cfg, model),save_dir, get_device())\n",
    "trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=X.XXX):   0%|          | 0/230 [00:00<?, ?it/s]/home/vykrishnan/private/Untitled Folder/optim.py:115: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Iter (loss=-1013.036): 100%|██████████| 230/230 [01:00<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 : Average Loss -470.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1046.926): 100%|██████████| 230/230 [01:00<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 : Average Loss -619.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1060.568): 100%|██████████| 230/230 [01:00<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 : Average Loss -650.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-812.154):   0%|          | 1/230 [00:00<01:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/3 : Average Loss -812.154\n",
      "The Total Steps have been reached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1060.263): 100%|██████████| 230/230 [01:00<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/3 : Average Loss -657.427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1060.543): 100%|██████████| 230/230 [01:00<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/3 : Average Loss -654.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1050.990): 100%|██████████| 230/230 [01:00<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/3 : Average Loss -651.203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-771.283):   1%|          | 2/230 [00:00<00:59,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/3 : Average Loss -789.579\n",
      "The Total Steps have been reached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1049.748): 100%|██████████| 230/230 [01:00<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/3 : Average Loss -653.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1036.728): 100%|██████████| 230/230 [01:00<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/3 : Average Loss -665.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1066.565): 100%|██████████| 230/230 [01:00<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/3 : Average Loss -668.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-496.924):   1%|▏         | 3/230 [00:00<00:59,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/3 : Average Loss -690.969\n",
      "The Total Steps have been reached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1056.080): 100%|██████████| 230/230 [01:00<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/3 : Average Loss -667.721\n"
     ]
    }
   ],
   "source": [
    "trainer.train(get_loss, n_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(data_iter, model):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            input_ids, segment_ids, input_mask, label_id = batch\n",
    "            feats = model(input_ids.cuda(), segment_ids.cuda(), input_mask.cuda())\n",
    "            all_features.append(feats.cpu())  \n",
    "            all_labels.append(label_id.cpu())\n",
    "    return torch.cat(all_features), torch.cat(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get new features and labels for training data\n",
    "x_train, y_train = get_features_and_labels(train_data_iter, model)\n",
    "\n",
    "x_test, y_test = get_features_and_labels(test_data_iter, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000, -1.0000,  1.0000,  ..., -1.0000, -1.0000,  1.0000],\n",
       "        [ 1.0000,  1.0000, -1.0000,  ...,  1.0000,  1.0000, -1.0000],\n",
       "        [-1.0000, -1.0000,  1.0000,  ..., -1.0000, -1.0000,  1.0000],\n",
       "        ...,\n",
       "        [-1.0000, -1.0000,  1.0000,  ..., -1.0000, -1.0000,  1.0000],\n",
       "        [-1.0000, -1.0000,  1.0000,  ..., -1.0000, -1.0000,  1.0000],\n",
       "        [ 1.0000,  1.0000, -1.0000,  ...,  1.0000,  1.0000, -1.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(features, labels,classifier):\n",
    "    if classifier=='svm':\n",
    "        svm = SVC(kernel='linear',random_state=42)\n",
    "        svm.fit(features, labels)\n",
    "        return svm\n",
    "    elif classifier=='rf':\n",
    "        rf = RandomForestClassifier(random_state = 42)\n",
    "        rf.fit(features, labels)\n",
    "        return rf\n",
    "    elif classifier=='boosting':\n",
    "        adb = AdaBoostClassifier(n_estimators=100, random_state = 42)\n",
    "        adb.fit(features, labels)\n",
    "        return adb\n",
    "    else:\n",
    "        gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "        gb.fit(features,labels)\n",
    "        return gb\n",
    "    \n",
    "\n",
    "def evaluate_classifier(classifier, features, labels):\n",
    "    predictions = classifier.predict(features)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.8382352941176471\n",
      "Random Forest Accuracy: 0.8406862745098039\n",
      "AdaBoost Accuracy: 0.8480392156862745\n",
      "Gradient Boosting Accuracy: 0.8357843137254902\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = train_classifier(x_train, y_train, 'svm')\n",
    "rf_classifier = train_classifier(x_train, y_train, 'rf')\n",
    "adb_classifier = train_classifier(x_train, y_train, 'boosting')\n",
    "gb_classifier = train_classifier(x_train, y_train, 'gb')\n",
    "\n",
    "svm_accuracy = evaluate_classifier(svm_classifier, x_test, y_test)\n",
    "rf_accuracy = evaluate_classifier(rf_classifier,  x_test, y_test)\n",
    "adb_accuracy = evaluate_classifier(adb_classifier, x_test, y_test)\n",
    "gb_accuracy = evaluate_classifier(gb_classifier, x_test, y_test)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"AdaBoost Accuracy:\", adb_accuracy)\n",
    "print(\"Gradient Boosting Accuracy:\", gb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda (1 GPUs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier_feats(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "trainer = train.Trainer(cfg, model, train_data_iter, test_data_iter, optim.optim4GPU(cfg, model),save_dir, get_device())\n",
    "trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomp_model = Classifier_feats(model_cfg, len(TaskDataset.labels)).cuda()\n",
    "decomp_trainer = train.Trainer(cfg, decomp_model, train_data_iter, test_data_iter, optim.optim4GPU(cfg, decomp_model),save_dir,'cuda')\n",
    "decomp_trainer.model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"Implementation of the gelu activation function by Hugging Face\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCCompression(nn.Module):\n",
    "    def __init__(self, cfg, k):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.fc1_u = nn.Linear(768, k, bias = False)\n",
    "        self.fc1_vs = nn.Linear(k, 3072)\n",
    "        \n",
    "        self.fc2_u = nn.Linear(3072, k)\n",
    "        self.fc2_vs = nn.Linear(k, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = gelu(self.fc1_vs(self.fc1_u(x)))\n",
    "        out = self.fc2_vs(self.fc2_u(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_matrix(mat, k):\n",
    "    U, s, VT = np.linalg.svd(mat, full_matrices=False)\n",
    "    Sigma = np.diag(s[:k])\n",
    "    U_truncated = U[:, :k]\n",
    "    VT_truncated = VT[:k, :]\n",
    "    return U_truncated, Sigma, VT_truncated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomp_model_func(decomp_trainer, rank):\n",
    "    for b in range(len(trainer.model.transformer.blocks)):\n",
    "        \n",
    "        block = trainer.model.transformer.blocks[b]\n",
    "        target_block = decomp_trainer.model.transformer.blocks[b]\n",
    "\n",
    "        target_block.pwff = FCCompression(model_cfg, rank[b])\n",
    "        \n",
    "        fc1_U, fc1_Sigma, fc1_V = decompose_matrix(block.pwff.fc1.weight.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.pwff.fc1_u.weight.data = torch.from_numpy(fc1_U).T.cuda().contiguous()\n",
    "        target_block.pwff.fc1_vs.weight.data = torch.from_numpy(fc1_Sigma@fc1_V).T.cuda().contiguous()\n",
    "        target_block.pwff.fc1_vs.bias.data = block.pwff.fc1.bias\n",
    "        \n",
    "        fc2_U, fc2_Sigma, fc2_V = decompose_matrix(block.pwff.fc2.weight.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.pwff.fc2_u.weight.data = torch.from_numpy(fc2_U).T.cuda().contiguous()\n",
    "        target_block.pwff.fc2_vs.weight.data = torch.from_numpy(fc2_Sigma@fc2_V).T.cuda().contiguous()\n",
    "        target_block.pwff.fc2_vs.bias.data = block.pwff.fc2.bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [300]*12\n",
    "rank[0] = rank[1] = 384\n",
    "decomp_model_func(decomp_trainer, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48893229166666663\n"
     ]
    }
   ],
   "source": [
    "compression = 0\n",
    "for r in rank:\n",
    "    compression += (3072*r + 768*r)\n",
    "\n",
    "compression = 1 - compression/ (12*768*3072)\n",
    "print(compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=X.XXX):   0%|          | 0/230 [00:00<?, ?it/s]/home/vykrishnan/private/Untitled Folder/optim.py:115: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Iter (loss=-920.346): 100%|██████████| 230/230 [00:51<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 : Average Loss -211.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1028.436): 100%|██████████| 230/230 [00:51<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 : Average Loss -374.427\n"
     ]
    }
   ],
   "source": [
    "decomp_trainer.train(get_loss, n_epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and labels for training data\n",
    "x_train, y_train = get_features_and_labels(train_data_iter, decomp_model)\n",
    "\n",
    "# Get features and labels for testing data\n",
    "x_test, y_test = get_features_and_labels(test_data_iter, decomp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.7843137254901961\n",
      "Random Forest Accuracy: 0.8186274509803921\n",
      "AdaBoost Accuracy: 0.8014705882352942\n",
      "Gradient Boosting Accuracy: 0.821078431372549\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = train_classifier(x_train, y_train, 'svm')\n",
    "rf_classifier = train_classifier(x_train, y_train, 'rf')\n",
    "adb_classifier = train_classifier(x_train, y_train, 'boosting')\n",
    "gb_classifier = train_classifier(x_train, y_train, 'gb')\n",
    "\n",
    "svm_accuracy = evaluate_classifier(svm_classifier, x_test, y_test)\n",
    "rf_accuracy = evaluate_classifier(rf_classifier,  x_test, y_test)\n",
    "adb_accuracy = evaluate_classifier(adb_classifier, x_test, y_test)\n",
    "gb_accuracy = evaluate_classifier(gb_classifier, x_test, y_test)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"AdaBoost Accuracy:\", adb_accuracy)\n",
    "print(\"Gradient Boosting Accuracy:\", gb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadProjection(nn.Module):\n",
    "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
    "    def __init__(self, cfg, k):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
    "        \n",
    "        self.proj_q_u = nn.Linear(768,k)\n",
    "        self.proj_q_vs = nn.Linear(k, 768, bias = False)\n",
    "        \n",
    "        self.proj_k_u = nn.Linear(768,k)\n",
    "        self.proj_k_vs = nn.Linear(k, 768, bias = False)\n",
    "        \n",
    "        self.proj_v_u = nn.Linear(768,k)\n",
    "        self.proj_v_vs = nn.Linear(k, 768, bias = False)\n",
    "            \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        \"\"\"\n",
    "    \n",
    "        q = self.proj_q_u(self.proj_q_vs(x))\n",
    "        k = self.proj_k_u(self.proj_k_vs(x))\n",
    "        v = self.proj_v_u(self.proj_v_vs(x))\n",
    "        \n",
    "        q, k, v = (split_last(x, (12, -1)).transpose(1, 2) for x in [q, k, v])\n",
    "        \n",
    "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :].float()\n",
    "            scores -= 10000.0 * (1.0 - mask)\n",
    "\n",
    "        scores = self.drop(F.softmax(scores, dim=-1))\n",
    "        h = (scores @ v).transpose(1, 2).contiguous()\n",
    "        h = merge_last(h, 2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomp_proj(decom_trainer, rank):\n",
    "    for b in range(len(trainer.model.transformer.blocks)):\n",
    "        \n",
    "        block = trainer.model.transformer.blocks[b]\n",
    "        target_block = decomp_trainer.model.transformer.blocks[b]\n",
    "\n",
    "        target_block.attn = MultiHeadProjection(model_cfg, rank[b]) #, thres = decomp_trainer.model.thres\n",
    "        \n",
    "        proj_q_u, proj_q_s, proj_q_v = decompose_matrix(block.attn.proj_q.weight.T.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.attn.proj_q_u.weight.data = torch.from_numpy(proj_q_u).cuda().contiguous()\n",
    "        target_block.attn.proj_q_vs.weight.data = torch.from_numpy(proj_q_s@proj_q_v).cuda().contiguous()\n",
    "        target_block.attn.proj_q_u.bias.data = block.attn.proj_q.bias\n",
    "        \n",
    "        proj_k_u, proj_k_s, proj_k_v = decompose_matrix(block.attn.proj_k.weight.T.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.attn.proj_k_u.weight.data = torch.from_numpy(proj_k_u).cuda().contiguous()\n",
    "        target_block.attn.proj_k_vs.weight.data = torch.from_numpy(proj_k_s@proj_k_v).cuda().contiguous()\n",
    "        target_block.attn.proj_k_u.bias.data = block.attn.proj_k.bias\n",
    "        \n",
    "        proj_v_u, proj_v_s, proj_v_v = decompose_matrix(block.attn.proj_v.weight.T.detach().cpu().numpy().T, rank[b])\n",
    "        target_block.attn.proj_v_u.weight.data = torch.from_numpy(proj_v_u).cuda().contiguous()\n",
    "        target_block.attn.proj_v_vs.weight.data = torch.from_numpy(proj_v_s@proj_v_v).cuda().contiguous()\n",
    "        target_block.attn.proj_v_u.bias.data = block.attn.proj_v.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = [225]*12\n",
    "rank[0] = rank[1] = 384\n",
    "decomp_proj(decomp_trainer, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34505208333333337\n"
     ]
    }
   ],
   "source": [
    "cr = [(2*x*768) / (768*768) for x in rank]\n",
    "print(1 - np.mean(cr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1033.359): 100%|██████████| 230/230 [00:47<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 : Average Loss -241.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter (loss=-1049.900): 100%|██████████| 230/230 [00:47<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 : Average Loss -343.038\n"
     ]
    }
   ],
   "source": [
    "decomp_trainer.train(get_loss, n_epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and labels for training data\n",
    "x_train, y_train = get_features_and_labels(train_data_iter, decomp_model)\n",
    "\n",
    "# Get features and labels for testing data\n",
    "x_test, y_test = get_features_and_labels(test_data_iter, decomp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9963,  0.9951, -0.9919,  ...,  0.9969,  0.9944, -0.9975],\n",
       "        [ 0.9955,  0.9959, -0.9959,  ...,  0.9968,  0.9969, -0.9974],\n",
       "        [-0.9912, -0.9946,  0.9927,  ..., -0.9936, -0.9923,  0.9927],\n",
       "        ...,\n",
       "        [-0.9974, -0.9979,  0.9959,  ..., -0.9949, -0.9978,  0.9974],\n",
       "        [-0.9960, -0.9965,  0.9932,  ..., -0.9931, -0.9963,  0.9963],\n",
       "        [ 0.9966,  0.9971, -0.9973,  ...,  0.9976,  0.9977, -0.9981]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.7745098039215687\n",
      "Random Forest Accuracy: 0.7818627450980392\n",
      "AdaBoost Accuracy: 0.7745098039215687\n",
      "Gradient Boosting Accuracy: 0.7794117647058824\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = train_classifier(x_train, y_train, 'svm')\n",
    "rf_classifier = train_classifier(x_train, y_train, 'rf')\n",
    "adb_classifier = train_classifier(x_train, y_train, 'boosting')\n",
    "gb_classifier = train_classifier(x_train, y_train, 'gb')\n",
    "\n",
    "svm_accuracy = evaluate_classifier(svm_classifier, x_test, y_test)\n",
    "rf_accuracy = evaluate_classifier(rf_classifier,  x_test, y_test)\n",
    "adb_accuracy = evaluate_classifier(adb_classifier, x_test, y_test)\n",
    "gb_accuracy = evaluate_classifier(gb_classifier, x_test, y_test)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"AdaBoost Accuracy:\", adb_accuracy)\n",
    "print(\"Gradient Boosting Accuracy:\", gb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
